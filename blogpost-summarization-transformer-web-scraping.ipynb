{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing dependencies","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nfrom bs4 import BeautifulSoup\nimport requests","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:09:16.654024Z","iopub.execute_input":"2023-12-27T14:09:16.654755Z","iopub.status.idle":"2023-12-27T14:09:39.258499Z","shell.execute_reply.started":"2023-12-27T14:09:16.654703Z","shell.execute_reply":"2023-12-27T14:09:39.256980Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Load summarization pipeline","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"summarizer = pipeline('summarization')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:09:43.036558Z","iopub.execute_input":"2023-12-27T14:09:43.037797Z","iopub.status.idle":"2023-12-27T14:10:18.088327Z","shell.execute_reply.started":"2023-12-27T14:09:43.037739Z","shell.execute_reply":"2023-12-27T14:10:18.086905Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8fc3318cec641099985ff73e71c9ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c50e52a38a4c75a760ce9dcd5d82e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ab0ab3a0e294a5cb1b59c303a9545da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68df9042119e409b8f923cd7b693dd9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3f7a4c33e14887bd65da54c94a2a8f"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Get Blog Post","metadata":{}},{"cell_type":"code","source":"URL = \"https://www.analyticsvidhya.com/blog/2023/08/fine-tuning-large-language-models/\"","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:43:40.621610Z","iopub.execute_input":"2023-12-27T14:43:40.622693Z","iopub.status.idle":"2023-12-27T14:43:40.627823Z","shell.execute_reply.started":"2023-12-27T14:43:40.622652Z","shell.execute_reply":"2023-12-27T14:43:40.626891Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"r = requests.get(URL)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:43:42.818537Z","iopub.execute_input":"2023-12-27T14:43:42.819005Z","iopub.status.idle":"2023-12-27T14:43:43.775206Z","shell.execute_reply.started":"2023-12-27T14:43:42.818967Z","shell.execute_reply":"2023-12-27T14:43:43.774013Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"soup = BeautifulSoup(r.text, 'html.parser')\nresults = soup.find_all(['h1', 'p'])\ntext = [result.text for result in results]\nArticle = ' '.join(text)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:43:45.374102Z","iopub.execute_input":"2023-12-27T14:43:45.374498Z","iopub.status.idle":"2023-12-27T14:43:45.503240Z","shell.execute_reply.started":"2023-12-27T14:43:45.374466Z","shell.execute_reply":"2023-12-27T14:43:45.502092Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Article","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:43:54.067585Z","iopub.execute_input":"2023-12-27T14:43:54.068212Z","iopub.status.idle":"2023-12-27T14:43:54.075313Z","shell.execute_reply.started":"2023-12-27T14:43:54.068178Z","shell.execute_reply":"2023-12-27T14:43:54.073968Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'\\n A Comprehensive Guide to Fine-Tuning Large Language Models Over the past few years, the landscape of natural language processing (NLP) has undergone a remarkable transformation, all thanks to the advent of large language models. These sophisticated models have opened the doors to a wide array of applications, ranging from language translation to sentiment analysis and even the creation of intelligent chatbots. But their versatility sets these models apart; fine-tuning them to tackle specific tasks and domains has become a standard practice, unlocking their true potential and elevating their performance to new heights. In this comprehensive guide, we’ll delve into the world of fine-tuning large language models, covering everything from the basics to advanced. This article was published as a part of the\\xa0Data Science Blogathon. Pre-trained language models are large neural networks trained on vast corpora of text data, usually sourced from the internet. The training process involves predicting missing words or tokens in a given sentence or sequence, which imbues the model with a profound understanding of grammar, context, and semantics. By processing billions of sentences, these models can grasp the intricacies of language and effectively capture its nuances. Examples of popular pre-trained language models include BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pre-trained Transformer 3), RoBERTa (A Robustly Optimized BERT Pretraining Approach), and many more. These models are known for their ability to perform tasks such as text generation, sentiment classification, and language understanding at an impressive level of proficiency. Let’s discuss one of the language models in detail. GPT-3 Generative Pre-trained Transformer 3 is a ground-breaking language model architecture that has transformed natural language generation and understanding. The Transformer model is the foundation for the GPT-3 architecture, which incorporates several parameters to produce exceptional performance. A stack of Transformer encoder layers makes up GPT-3. Multi-head self-attention mechanisms and feed-forward neural networks make up each layer. While the feed-forward networks process and transform the encoded representations, the attention mechanism enables the model to recognize dependencies and relationships between words. The main innovation of GPT-3 is its enormous size, which allows it to capture a huge amount of language knowledge thanks to its astounding 175 billion parameters. You can use the OpenAI API to interact with the GPT- 3 model of openAI. Here is an example of text generation using GPT-3. Here’s the twist: while pre-trained language models are prodigious, they are not inherently experts in any specific task. They may have an incredible grasp of language, but they need some fine-tuning in tasks like sentiment analysis, language translation, or answering questions about specific domains. Fine-tuning is like providing a finishing touch to these versatile models. Imagine having a multi-talented friend who excels in various areas, but you need them to master one particular skill for a special occasion. You would give them some specific training in that area, right? That’s precisely what we do with pre-trained language models during fine-tuning. Fine-tuning involves training the pre-trained model on a smaller, task-specific dataset. This new dataset is labeled with examples relevant to the target task. By exposing the model to these labeled examples, it can adjust its parameters and internal representations to become well-suited for the target task. While pre-trained language models are remarkable, they are not task-specific by default. Fine-tuning is adapting these general-purpose models to perform specialized tasks more accurately and efficiently. When we encounter a specific NLP task like sentiment analysis for customer reviews or question-answering for a particular domain, we need to fine-tune the pre-trained model to understand the nuances of that specific task and domain. The benefits of fine-tuning are manifold. Firstly, it leverages the knowledge learned during pre-training, saving substantial time and computational resources that would otherwise be required to train a model from scratch. Secondly, fine-tuning allows us to perform better on specific tasks, as the model is now attuned to the intricacies and nuances of the domain it was fine-tuned for. The fine-tuning process typically involves feeding the task-specific dataset to the pre-trained model and adjusting its parameters through backpropagation. The goal is to minimize the loss function, which measures the difference between the model’s predictions and the ground-truth labels in the dataset. This fine-tuning process updates the model’s parameters, making it more specialized for your target task. Here we will walk through the process of fine-tuning a large language model for sentiment analysis. We’ll use the Hugging Face Transformers library, which provides easy access to pre-trained models and utilities for fine-tuning. Step 1: Load the Pre-trained Language Model and Tokenizer The first step is to load the pre-trained language model and its corresponding tokenizer. For this example, we’ll use the ‘distillery-base-uncased’ model, a lighter version of BERT. Step 2: Prepare the Sentiment Analysis Dataset We need a labeled dataset with text samples and corresponding sentiments for sentiment analysis. Let’s create a small dataset for illustration purposes: Next, we’ll use the tokenizer to convert the text samples into token IDs, and attention masks the model requires. Step 3: Add a Custom Classification Head The pre-trained language model itself doesn’t include a classification head. We must add one to the model to perform sentiment analysis. In this case, we’ll add a simple linear layer. Step 4: Fine-Tune the Model With the custom classification head in place, we can now fine-tune the model on the sentiment analysis dataset. We’ll use the AdamW optimizer and CrossEntropyLoss as the loss function. Instruction fine-tuning is a specialized technique to tailor large language models to perform specific tasks based on explicit instructions. While traditional fine-tuning involves training a model on task-specific data, instruction fine-tuning goes further by incorporating high-level instructions or demonstrations to guide the model’s behavior. This approach allows developers to specify desired outputs, encourage certain behaviors, or achieve better control over the model’s responses. In this comprehensive guide, we will explore the concept of instruction fine-tuning and its implementation step-by-step. What if we could go beyond traditional fine-tuning and provide explicit instructions to guide the model’s behavior? Instruction fine-tuning does that, offering a new level of control and precision over model outputs. Here we will explore the process of instruction fine-tuning large language models for sentiment analysis. Step 1: Load the Pre-trained Language Model and Tokenizer To begin, let’s load the pre-trained language model and its tokenizer. We’ll use GPT-3, a state-of-the-art language model, for this example. Step 2: Prepare the Instruction Data and Sentiment Analysis Dataset For instruction fine-tuning, we need to augment the sentiment analysis dataset with explicit instructions for the model. Let’s create a small dataset for demonstration: Next, let’s tokenize the texts, sentiments, and instructions using the tokenizer: Step 3: Customize the Model Architecture with Instructions To incorporate instructions during fine-tuning, we need to customize the model architecture. We can do this by concatenating the instruction IDs with the input IDs: Step 4: Fine-Tune the Model with Instructions With the instructions incorporated, we can now fine-tune the GPT-3 model on the augmented dataset. During fine-tuning, the instructions will guide the model’s sentiment analysis behavior. Instruction fine-tuning takes the power of traditional fine-tuning to the next level, allowing us to control the behavior of large language models precisely. By providing explicit instructions, we can guide the model’s output and achieve more accurate and tailored results. Standard fine-tuning involves training a model on a labeled dataset, honing its abilities to perform specific tasks effectively. But if we want to provide explicit instructions to guide the model’s behavior, instruction finetuning comes into play that offers unparalleled control and adaptability. Here are the critical differences between instruction finetuning and standard finetuning. As we sail into the world of fine-tuning, we encounter the perilous challenge of catastrophic forgetting. This phenomenon occurs when the model’s fine-tuning on a new task erases or ‘forgets’ the knowledge gained during pre-training. The model loses its understanding of the broader language structure as it focuses solely on the new task. Imagine our language model as a ship’s cargo hold filled with various knowledge containers, each representing different linguistic nuances. During pre-training, these containers are carefully filled with language understanding. The ship’s crew rearranges the containers when we approach a new task and begin fine-tuning. They empty some to make space for new task-specific knowledge. Unfortunately, some original knowledge is lost, leading to catastrophic forgetting. To navigate the waters of catastrophic forgetting, we need strategies to safeguard the valuable knowledge captured during pre-training. There are two possible approaches. Here we gradually introduce the new task to the model. Initially, the model focuses on pre-training knowledge and slowly incorporates the new task data, minimizing the risk of catastrophic forgetting. Multitask instruction fine-tuning embraces a new paradigm by simultaneously training language models on multiple tasks. Instead of fine-tuning the model for one task at a time, we provide explicit instructions for each task, guiding the model’s behavior during fine-tuning. Here we freeze certain layers of the model during fine-tuning. By freezing early layers responsible for fundamental language understanding, we preserve the core knowledge while only fine-tuning later layers for the specific task. Memory is necessary for full fine-tuning to store the model and several other training-related parameters. You must be able to allocate memory for optimizer states, gradients, forward activations, and temporary memory throughout the training process, even if your computer can hold the model weight of hundreds of gigabytes for the largest models. These extra parts may be much bigger than the model and quickly outgrow the capabilities of consumer hardware. Parameter-efficient fine-tuning techniques only update a small subset of parameters instead of full fine-tuning, which updates every model weight during supervised learning. Some path techniques concentrate on fine-tuning a portion of existing model parameters, such as specific layers or components, while freezing the majority of model weights. Other methods add a few new parameters or layers and only fine-tune the new components; they do not affect the original model weights. Most, if not all, LLM weights are kept frozen using PEFT. As a result, compared to the original LLM, there are significantly fewer trained parameters. PEFT empowers parameter-efficient models with impressive performance, revolutionizing the landscape of NLP. Here are a few reasons why we use PEFT. While freezing most pre-trained LLMs, PEFT only approaches fine-tuning a few model parameters, significantly lowering the computational and storage costs. This also resolves the problem of catastrophic forgetting, which was seen during LLMs’ full fine-tuning. In low-data regimes, PEFT approaches have also been demonstrated to be superior to fine-tuning and to better generalize to out-of-domain scenarios. Let’s load the opt-6.7b model here; its weights on the Hub are roughly 13GB in half-precision( float16). It will require about 7GB of memory if we load them in 8-bit. Let’s freeze all our layers and cast the layer norm in float32 for stability before applying some post-processing to the 8-bit model to enable training. We also cast the final layer’s output in float32 for the same reasons. Load a\\xa0PeftModel, we will use low-rank adapters (LoRA) using the get_peft_model\\xa0utility function from\\xa0Peft. The function calculates and prints the total number of trainable parameters and all parameters in a given model. Along with the percentage of trainable parameters, providing an overview of the model’s complexity and resource requirements for training. This uses the Peft library to create a LoRA model with specific configuration settings, including dropout, bias, and task type. It then obtains the trainable parameters of the model and prints the total number of trainable parameters and all parameters, along with the percentage of trainable parameters. This uses the Hugging Face Transformers and Datasets libraries to train a language model on a given dataset. It utilizes the ‘transformers.Trainer’ class to define the training setup, including batch size, learning rate, and other training-related configurations and then trains the model on the specified dataset. We will look closer at some exciting real-world use cases of fine-tuning large language models, where NLP advancements are transforming industries and empowering innovative solutions. In the real world, fine-tuning large language models has found applications across diverse industries, empowering businesses and researchers to harness the capabilities of NLP for a wide range of tasks, leading to enhanced efficiency, improved decision-making, and enriched user experiences. Fine-tuning large language models has emerged as a powerful technique to adapt these pre-trained models to specific tasks and domains. As the field of NLP advances, fine-tuning will remain crucial to developing cutting-edge language models and applications. This comprehensive guide has taken us on an enlightening journey through the world of fine-tuning large language models. We started by understanding the significance of fine-tuning, which complements pre-training and empowers language models to excel at specific tasks. Choosing the right pre-trained model is crucial, and we explored popular models. We dived into advanced techniques like multitask fine-tuning, parameter-efficient fine-tuning, and instruction fine-tuning, which push the boundaries of efficiency and control in NLP. Additionally, we explored real-world applications, witnessing how fine-tuned models revolutionize sentiment analysis, language translation, virtual assistants, medical analysis, financial predictions, and more. With the power of fine-tuning, we navigate the vast ocean of language with precision and creativity, transforming how we interact with and understand the world of text. So, embrace the possibilities and unleash the full potential of language models through fine-tuning, where the future of NLP is shaped with each finely tuned model. A1:\\xa0 Fine-tuning is adapting pre-trained language models to specific tasks and domains. It complements pre-training and enables models to excel in particular contexts, making them more powerful and effective for real-world applications. A2:\\xa0Multitask fine-tuning involves training a model on multiple related tasks simultaneously, enhancing its ability to transfer knowledge across tasks. Instruction fine-tuning introduces prompts or instructions during training, allowing fine-grained control over the model’s behavior. A3:\\xa0Parameter-efficient fine-tuning reduces the computational resources required, making it more accessible for low-resource environments while maintaining comparable performance to standard fine-tuning. A4:\\xa0While fine-tuning can lead to overfitting on small datasets, techniques like early stopping, dropout, and data augmentation can mitigate this risk and promote generalization to new data. A5:\\xa0In scenarios with limited labeled data, transfer learning from related tasks or leveraging pre-training on similar datasets can help improve the model’s performance and adaptability. Also, few-shot learning and data augmentation techniques can be useful for low-resource scenarios. The media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion.\\xa0  Life2vec AI Accurately Predicts the Future! Top 12 Data Science Leaders to Follow in 2024 6 Easy Ways to Access ChatGPT-4 for Free\\xa0 Building an LLM Model using Google Gemini API © Copyright 2013-2023 Analytics Vidhya.  A verification link has been sent to your email id   If you have not recieved the link please goto\\nSign Up  page again\\n This email id is not registered with us. Please enter your registered email id.'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Chunk text","metadata":{}},{"cell_type":"code","source":"max_chunk =500","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:44:15.687576Z","iopub.execute_input":"2023-12-27T14:44:15.687978Z","iopub.status.idle":"2023-12-27T14:44:15.693153Z","shell.execute_reply.started":"2023-12-27T14:44:15.687947Z","shell.execute_reply":"2023-12-27T14:44:15.691924Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"Article = Article.replace('.','.<eos>')\nArticle = Article.replace('?','?<eos>')\nArticle = Article.replace('!','!<eos>')","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:44:17.679458Z","iopub.execute_input":"2023-12-27T14:44:17.680380Z","iopub.status.idle":"2023-12-27T14:44:17.685032Z","shell.execute_reply.started":"2023-12-27T14:44:17.680340Z","shell.execute_reply":"2023-12-27T14:44:17.684112Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"sentences = Article.split('<eos>')\ncurrent_chunk = 0\nchunk =[]\n\nfor sentence in sentences:\n    if len(chunk) ==current_chunk +1:\n        if len(chunk[current_chunk])+len(sentence.split(' '))<=max_chunk:\n            chunk[current_chunk].extend(sentence.split(' '))\n        else:\n            current_chunk +=1\n            chunk.append(sentence.split(' '))\n    else:\n        print(current_chunk)\n        chunk.append(sentence.split(' '))\n\nfor chunk_id in range(len(chunk)):\n    chunk[chunk_id] = ' '.join(chunk[chunk_id])","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:44:21.652191Z","iopub.execute_input":"2023-12-27T14:44:21.652619Z","iopub.status.idle":"2023-12-27T14:44:21.664131Z","shell.execute_reply.started":"2023-12-27T14:44:21.652585Z","shell.execute_reply":"2023-12-27T14:44:21.662835Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"len(chunk)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:44:26.255911Z","iopub.execute_input":"2023-12-27T14:44:26.257032Z","iopub.status.idle":"2023-12-27T14:44:26.263872Z","shell.execute_reply.started":"2023-12-27T14:44:26.256993Z","shell.execute_reply":"2023-12-27T14:44:26.262834Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"markdown","source":"### Summarize text","metadata":{}},{"cell_type":"code","source":"res = summarizer(chunk , max_length =120 ,min_length =30,do_sample =False)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:46:25.716434Z","iopub.execute_input":"2023-12-27T14:46:25.717202Z","iopub.status.idle":"2023-12-27T14:47:16.630176Z","shell.execute_reply.started":"2023-12-27T14:46:25.717164Z","shell.execute_reply":"2023-12-27T14:47:16.628973Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"res[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:47:42.296552Z","iopub.execute_input":"2023-12-27T14:47:42.297116Z","iopub.status.idle":"2023-12-27T14:47:42.305466Z","shell.execute_reply.started":"2023-12-27T14:47:42.297074Z","shell.execute_reply":"2023-12-27T14:47:42.304284Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"{'summary_text': ' Pre-trained language models are large neural networks trained on vast corpora of text data . These models are known for their ability to perform tasks such as text generation, sentiment classification, and language understanding at an impressive level of proficiency . Fine-tuning is like providing a finishing touch to these versatile models .'}"},"metadata":{}}]},{"cell_type":"code","source":"text = ' '.join([summary['summary_text'] for summary in res])","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:48:04.164096Z","iopub.execute_input":"2023-12-27T14:48:04.165105Z","iopub.status.idle":"2023-12-27T14:48:04.169780Z","shell.execute_reply.started":"2023-12-27T14:48:04.165067Z","shell.execute_reply":"2023-12-27T14:48:04.168677Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:48:32.084992Z","iopub.execute_input":"2023-12-27T14:48:32.085410Z","iopub.status.idle":"2023-12-27T14:48:32.093362Z","shell.execute_reply.started":"2023-12-27T14:48:32.085377Z","shell.execute_reply":"2023-12-27T14:48:32.091791Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"' Pre-trained language models are large neural networks trained on vast corpora of text data . These models are known for their ability to perform tasks such as text generation, sentiment classification, and language understanding at an impressive level of proficiency . Fine-tuning is like providing a finishing touch to these versatile models .  Fine-tuning involves training the pre-trained model on a smaller, task-specific dataset . This new dataset is labeled with examples relevant to the target task . By exposing the model to these labeled examples, it can adjust its parameters and internal representations to become well-suited for a specific task .  In this comprehensive guide, we will explore the concept of instruction fine-tuning and its implementation step-by-step . We will use GPT-3, a state-of-the-art language model, for this example . The instructions will guide the model’s sentiment analysis behavior .  Multitask instruction fine-tuning embraces a new paradigm by training language models on multiple tasks . By freezing early layers responsible for fundamental language understanding, we preserve core knowledge . Memory is necessary to store the model and several other training-related parameters . PEFT empowers parameter-efficient models with impressive performance .  Fine-tuning large language models has emerged as a powerful technique to adapt these pre-trained models to specific tasks and domains . We explore how fine-tuned models revolutionize sentiment analysis, language translation, virtual assistants, medical analysis, financial predictions, and more .  This email id is not registered with us.  Please enter your registered email id . A verification link has been sent to your email id   If you have not recieved the link please click here to sign up .'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Output to text file","metadata":{}},{"cell_type":"code","source":"with open('blogsummary.txt','w') as f:\n    f.write(text)","metadata":{"execution":{"iopub.status.busy":"2023-12-27T14:49:55.998671Z","iopub.execute_input":"2023-12-27T14:49:55.999149Z","iopub.status.idle":"2023-12-27T14:49:56.006258Z","shell.execute_reply.started":"2023-12-27T14:49:55.999108Z","shell.execute_reply":"2023-12-27T14:49:56.004488Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}